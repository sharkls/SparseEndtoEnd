# Copyright (c) 2024 SparseEnd2End. All rights reserved @author: Thomas Von Wu.
"""
Generate nuScenes format offline data for C++ testing.
This script extracts images and calibration parameters from nuScenes dataset
and saves them in a format that can be loaded by the C++ loadOfflineData function.
"""

import os
import numpy as np
import cv2
from tqdm import tqdm
from typing import Optional, Dict, List
import json

from tool.utils.config import read_cfg
from tool.trainer.utils import set_random_seed

from dataset.nuscenes_dataset import *
from dataset.dataloader_wrapper.dataloader_wrapper import dataloader_wrapper
from dataset.utils.scatter_gather import scatter
from dataset.pipeline.nuscenes import LoadMultiViewImageFromFiles

# å®šä¹‰å¿…è¦çš„å˜é‡
point_cloud_range = [-50.0, -50.0, -5.0, 50.0, 50.0, 3.0]
class_names = [
    "car", "truck", "bus", "trailer", "construction_vehicle", 
    "pedestrian", "motorcycle", "bicycle", "traffic_cone", "barrier"
]

# å®šä¹‰nuScenesæ ‡ç­¾åˆ°ç›®æ ‡æ ‡ç­¾çš„æ˜ å°„
nuscenes_to_target_mapping = {
    # è½¦è¾†ç±»åˆ«
    "vehicle.car": "car",
    "vehicle.truck": "truck", 
    "vehicle.bus.bendy": "bus",
    "vehicle.bus.rigid": "bus",
    "vehicle.trailer": "trailer",
    "vehicle.construction": "construction_vehicle",
    
    # è¡Œäºº
    "human.pedestrian.adult": "pedestrian",
    "human.pedestrian.child": "pedestrian", 
    "human.pedestrian.construction_worker": "pedestrian",
    "human.pedestrian.police_officer": "pedestrian",
    
    # ä¸¤è½®è½¦
    "vehicle.motorcycle": "motorcycle",
    "vehicle.bicycle": "bicycle",
    
    # å…¶ä»–
    "movable_object.trafficcone": "traffic_cone",
    "movable_object.barrier": "barrier",
}

def verify_nuscenes_dataset_access(config_path: str):
    """
    Verify that nuScenes dataset is accessible and original image files exist.
    
    Args:
        config_path: Path to the dataset config file
    
    Returns:
        bool: True if dataset is accessible, False otherwise
    """
    print("Verifying nuScenes dataset access...")
    
    try:
        # è¯»å–é…ç½®
        cfg = read_cfg(config_path)
        
        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
        dataset_cfg = cfg["data"]["val"].copy()
        data_root = dataset_cfg.get("data_root", "")
        ann_file = dataset_cfg.get("ann_file", "")
        
        print(f"Data root: {data_root}")
        print(f"Annotation file: {ann_file}")
        
        if not os.path.exists(data_root):
            print(f"âŒ Data root directory not found: {data_root}")
            return False
        
        if not os.path.exists(ann_file):
            print(f"âŒ Annotation file not found: {ann_file}")
            return False
        
        # å°è¯•åŠ è½½æ•°æ®é›†å¹¶æ£€æŸ¥åŸå§‹å›¾åƒæ–‡ä»¶
        dataset_type = dataset_cfg.pop("type")
        dataset_cfg["pipeline"] = [
            dict(type="LoadMultiViewImageFromFiles", to_float32=False),
        ]
        
        dataset = eval(dataset_type)(**dataset_cfg)
        
        # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬çš„åŸå§‹å›¾åƒæ–‡ä»¶
        for i in range(min(3, len(dataset))):
            data = dataset[i]
            img_filenames = data["img_filename"]
            
            print(f"Sample {i}: Found {len(img_filenames)} image files")
            for j, img_path in enumerate(img_filenames):
                if os.path.exists(img_path):
                    print(f"  âœ… Camera {j}: {img_path}")
                else:
                    print(f"  âŒ Camera {j}: File not found - {img_path}")
        
        print("âœ… nuScenes dataset access verified successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error verifying dataset access: {e}")
        return False


def build_module(cfg, default_args: Optional[Dict] = None):
    """Build module from config."""
    cfg2 = cfg.copy()
    if default_args is not None:
        
        for name, value in default_args.items():
            cfg2.setdefault(name, value)
    type = cfg2.pop("type")
    return eval(type)(**cfg2)


def generate_nuscenes_offline_data(config_path: str, output_path: str, num_frames: int = 3):
    """
    Generate nuScenes format offline data for C++ testing.
    ä¸010è„šæœ¬ä½¿ç”¨ç›¸åŒçš„æ•°æ®æºå’Œé…ç½®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
    
    Args:
        config_path: Path to the dataset config file
        output_path: Output directory for offline data
        num_frames: Number of frames to generate (é»˜è®¤3å¸§ï¼Œä¸010è„šæœ¬ä¸€è‡´)
    """
    print(f"Generating nuScenes offline data for {num_frames} frames...")
    print(f"Using same data source and configuration as 010 script")
    
    # è¯»å–é…ç½®
    cfg = read_cfg(config_path)
    # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„éšæœºç§å­
    set_random_seed(seed=1, deterministic=True)
    
    # æ‰“å°æ•°æ®é›†é…ç½®ä¿¡æ¯
    print(f"Dataset config: {cfg['data']['test']}")  # ä½¿ç”¨testé…ç½®ï¼Œä¸010è„šæœ¬ä¸€è‡´
    dataset_cfg = cfg["data"]["test"].copy()  # ä½¿ç”¨testé…ç½®ï¼Œä¸010è„šæœ¬ä¸€è‡´
    print(f"Data root: {dataset_cfg.get('data_root', 'Not specified')}")
    print(f"Annotation file: {dataset_cfg.get('ann_file', 'Not specified')}")
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
    data_root = dataset_cfg.get("data_root", "")
    if data_root and os.path.exists(data_root):
        print(f"âœ… Data root exists: {data_root}")
        # åˆ—å‡ºæ•°æ®æ ¹ç›®å½•ä¸‹çš„å†…å®¹
        try:
            data_root_contents = os.listdir(data_root)
            print(f"Data root contents: {data_root_contents[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
        except Exception as e:
            print(f"Error listing data root: {e}")
    else:
        print(f"âŒ Data root not found: {data_root}")
    
    # åˆ›å»ºæ•°æ®é›†ï¼Œä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„test_pipeline
    dataset_type = dataset_cfg.pop("type")
    print(f"Dataset type: {dataset_type}")
    
    # ç§»é™¤ä¸åº”è¯¥ä¼ é€’ç»™æ•°æ®é›†çš„å‚æ•°
    samples_per_gpu = dataset_cfg.pop("samples_per_gpu", 1)
    workers_per_gpu = dataset_cfg.pop("workers_per_gpu", 0)
    
    # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„test_pipeline
    if "pipeline" in dataset_cfg:
        # ä½¿ç”¨å®Œæ•´çš„test_pipelineï¼Œä¸010è„šæœ¬å®Œå…¨ä¸€è‡´
        dataset_cfg["pipeline"] = [
            dict(type="LoadMultiViewImageFromFiles", to_float32=True),
            dict(type="ResizeCropFlipImage"),
            dict(type="NormalizeMultiviewImage", **cfg.get("img_norm_cfg", {"mean": [0, 0, 0], "std": [1, 1, 1], "to_rgb": True})),
            dict(type="NuScenesSparse4DAdaptor"),
            dict(
                type="Collect",
                keys=["img", "timestamp", "lidar2img", "image_wh", "ori_img"],
                meta_keys=["lidar2global", "global2lidar", "timestamp"],
            ),
        ]
    
    print(f"Pipeline config: {dataset_cfg['pipeline']}")
    
    try:
        dataset = eval(dataset_type)(**dataset_cfg)
        print(f"âœ… Dataset created successfully, length: {len(dataset)}")
    except Exception as e:
        print(f"âŒ Error creating dataset: {e}")
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„é…ç½®
    dataloader = dataloader_wrapper(
        dataset,
        samples_per_gpu=samples_per_gpu,  # ä¸010è„šæœ¬ä¸€è‡´
        workers_per_gpu=workers_per_gpu,
        dist=False,
        shuffle=False,  # ä¸010è„šæœ¬ä¸€è‡´
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    camera_types = ["CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_FRONT_LEFT", 
                   "CAM_BACK", "CAM_BACK_LEFT", "CAM_BACK_RIGHT"]
    
    for cam_type in camera_types:
        cam_dir = os.path.join(output_path, "images", cam_type)
        os.makedirs(cam_dir, exist_ok=True)
    
    calib_dir = os.path.join(output_path, "calib")
    os.makedirs(calib_dir, exist_ok=True)
    
    # å¤„ç†æ•°æ®
    data_iter = dataloader.__iter__()
    
    for frame_idx in tqdm(range(num_frames), desc="Generating frames"):
        try:
            # è·å–æ•°æ® - ä¸010è„šæœ¬å®Œå…¨ç›¸åŒçš„æ–¹å¼
            data = next(data_iter)
            data = scatter(data, [0])[0]
            
            # è·å–æ•°æ®
            img_metas = data["img_metas"][0]
            lidar2img = data["lidar2img"].data[0].cpu().numpy()  # (1, 6, 4, 4) - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
            img = data["img"].data[0].cpu().numpy()  # (1, 6, 3, 256, 704) - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
            ori_img = data["ori_img"].data[0].cpu().numpy()  # (1, 6, 3, 900, 1600) - åŸå§‹å›¾åƒ
            
            print(f"\nFrame {frame_idx}: Processing {len(camera_types)} cameras")
            print(f"  ğŸ“Š ori_img shape: {ori_img.shape}")
            print(f"  ğŸ“Š img shape: {img.shape}")
            print(f"  ğŸ“Š lidar2img shape: {lidar2img.shape}")
            
            # ä¿å­˜åŸå§‹å›¾åƒæ•°æ®ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ - ä¸010è„šæœ¬æ ¼å¼å®Œå…¨ä¸€è‡´
            # 010è„šæœ¬ä¿å­˜æ ¼å¼: sample_{frame_idx}_ori_imgs_1*6*3*900*1600_uint8.bin
            ori_imgs_path = os.path.join(output_path, "calib", f"sample_{frame_idx}_ori_imgs_1*6*3*900*1600_uint8.bin")
            ori_img.astype(np.uint8).tofile(ori_imgs_path)
            print(f"  âœ… Saved ori_imgs binary: {ori_imgs_path}")
            
            # ä¿å­˜é¢„å¤„ç†å›¾åƒæ•°æ®ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ - ä¸010è„šæœ¬æ ¼å¼å®Œå…¨ä¸€è‡´
            # 010è„šæœ¬ä¿å­˜æ ¼å¼: sample_{frame_idx}_imgs_1*6*3*256*704_float32.bin
            imgs_path = os.path.join(output_path, "calib", f"sample_{frame_idx}_imgs_1*6*3*256*704_float32.bin")
            img.astype(np.float32).tofile(imgs_path)
            print(f"  âœ… Saved imgs binary: {imgs_path}")
            
            # åŒæ—¶ä¿å­˜JPGå›¾åƒæ–‡ä»¶ç”¨äºå¯è§†åŒ–
            for cam_idx, cam_type in enumerate(camera_types):
                cam_img = ori_img[0, cam_idx]
                print(f"  Camera {cam_type}: cam_img shape = {cam_img.shape}")
                
                # å¤„ç†ä¸åŒçš„å›¾åƒæ ¼å¼
                if cam_img.shape == (3, 900, 1600):
                    # ä¸‰é€šé“å½©è‰²å›¾åƒ
                    cam_img_cv = np.transpose(cam_img, (1, 2, 0))
                    cam_img_bgr = cv2.cvtColor(cam_img_cv, cv2.COLOR_RGB2BGR)
                elif cam_img.shape == (900, 1600):
                    # ç°åº¦å›¾åƒï¼Œè½¬æ¢ä¸ºä¸‰é€šé“
                    print(f"    âš ï¸ æ£€æµ‹åˆ°ç°åº¦å›¾åƒï¼Œè½¬æ¢ä¸ºä¸‰é€šé“")
                    cam_img_bgr = cv2.cvtColor(cam_img, cv2.COLOR_GRAY2BGR)
                elif cam_img.shape == (900, 1600, 3):
                    # å·²ç»æ˜¯BGRæ ¼å¼
                    cam_img_bgr = cam_img
                else:
                    print(f"    âš ï¸ æœªçŸ¥å›¾åƒæ ¼å¼: {cam_img.shape}ï¼Œå°è¯•è½¬æ¢ä¸ºä¸‰é€šé“")
                    # å°è¯•è½¬æ¢ä¸ºä¸‰é€šé“
                    if cam_img.ndim == 2:
                        cam_img_bgr = cv2.cvtColor(cam_img, cv2.COLOR_GRAY2BGR)
                    elif cam_img.ndim == 3 and cam_img.shape[2] == 3:
                        cam_img_bgr = cam_img
                    else:
                        raise ValueError(f"æ— æ³•å¤„ç†çš„å›¾åƒæ ¼å¼: {cam_img.shape}")
                
                target_img_path = os.path.join(output_path, "images", cam_type, f"{frame_idx}.jpg")
                cv2.imwrite(target_img_path, cam_img_bgr)
                print(f"  Camera {cam_type}: Saved to {target_img_path}")
                print(f"    ğŸ“Š Image stats: shape={cam_img_bgr.shape}, mean={cam_img_bgr.mean():.1f}, std={cam_img_bgr.std():.1f}")
            
            # ä¿å­˜lidar2imgæ ‡å®šå‚æ•° - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
            calib_path = os.path.join(output_path, "calib", f"lidar2img_{frame_idx}.bin")
            lidar2img.astype(np.float32).tofile(calib_path)
            print(f"  âœ… Saved lidar2img calibration for frame {frame_idx}: shape={lidar2img.shape}")
            
            # ä¿å­˜æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯
            if "timestamp" in data:
                timestamp = data["timestamp"].data[0].cpu().numpy()  # (1,)
                timestamp_path = os.path.join(output_path, "calib", f"timestamp_{frame_idx}.bin")
                timestamp.astype(np.float32).tofile(timestamp_path)
                print(f"  âœ… Saved timestamp for frame {frame_idx}: shape={timestamp.shape}")
            
            # ä¿å­˜global2lidarå˜æ¢çŸ©é˜µ
            if "global2lidar" in img_metas:
                global2lidar = img_metas["global2lidar"]
                global2lidar_path = os.path.join(output_path, "calib", f"global2lidar_{frame_idx}.bin")
                global2lidar.astype(np.float32).tofile(global2lidar_path)
                print(f"  âœ… Saved global2lidar for frame {frame_idx}: shape={global2lidar.shape}")
            
        except StopIteration:
            print(f"Dataset exhausted after {frame_idx} frames")
            break
        except Exception as e:
            print(f"Error processing frame {frame_idx}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"Successfully generated offline data in: {output_path}")
    print(f"Generated {num_frames} frames with {len(camera_types)} cameras each")
    print(f"Data format now matches 010 script requirements:")
    print(f"  - Images: (1, 6, 3, 256, 704) float32 (preprocessed)")
    print(f"  - Original images: (1, 6, 3, 900, 1600) uint8")
    print(f"  - Lidar2img: (1, 6, 4, 4) float32")
    print(f"  - Image_wh: (1, 6, 2) float32")
    print(f"  - Timestamp: (1,) float32")
    print(f"âœ… Data source consistency with 010 script:")
    print(f"  - Same config file: {config_path}")
    print(f"  - Same dataset: test (not val)")
    print(f"  - Same random seed: 1")
    print(f"  - Same pipeline: test_pipeline")
    print(f"  - Same frame count: {num_frames}")


def generate_lidar2img_calib_params(config, output_path: str, seed=1, num_frames=3):
    """
    Generate lidar2img calibration parameters for each frame separately.
    This function is adapted from 020.genernate_onboard_modelconfig_and_inputbin_file.py
    ä¸010è„šæœ¬ä½¿ç”¨ç›¸åŒçš„æ•°æ®æºå’Œé…ç½®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
    
    Args:
        config: Dataset configuration
        output_path: Output directory for calibration files
        seed: Random seed (é»˜è®¤1ï¼Œä¸010è„šæœ¬ä¸€è‡´)
        num_frames: Number of frames to generate (é»˜è®¤3å¸§ï¼Œä¸010è„šæœ¬ä¸€è‡´)
    """
    print(f"Generating lidar2img calibration parameters for {num_frames} frames...")
    print(f"Using same data source and configuration as 010 script")
    
    # åˆ›å»ºæ ‡å®šç›®å½•
    calib_dir = os.path.join(output_path, "calib")
    os.makedirs(calib_dir, exist_ok=True)
    
    # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„testé…ç½®
    dataset_cfg = config.copy()["data"]["test"].copy()  # ä½¿ç”¨testé…ç½®ï¼Œä¸010è„šæœ¬ä¸€è‡´
    
    # ç§»é™¤ä¸åº”è¯¥ä¼ é€’ç»™æ•°æ®é›†çš„å‚æ•°
    samples_per_gpu = dataset_cfg.pop("samples_per_gpu", 1)
    workers_per_gpu = dataset_cfg.pop("workers_per_gpu", 0)
    
    dataset_type = dataset_cfg.pop("type")
    dataset = eval(dataset_type)(**dataset_cfg)

    dataloader = dataloader_wrapper(
        dataset,
        samples_per_gpu=samples_per_gpu,
        workers_per_gpu=workers_per_gpu,
        seed=seed,  # ä¸010è„šæœ¬ä¸€è‡´
        dist=False,
        shuffle=False,  # ä¸010è„šæœ¬ä¸€è‡´
    )
    data_iter = dataloader.__iter__()

    for frame_idx in tqdm(range(num_frames), desc="Generating lidar2img calibration"):
        try:
            data = next(data_iter)
            data = scatter(data, [0])[0]
            lidar2img = data["lidar2img"][0].cpu().numpy()  # (1, 6, 4, 4) - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
            
            # æ¯å¸§å•ç‹¬ä¿å­˜ä¸ºä¸€ä¸ªbinæ–‡ä»¶
            calib_path = os.path.join(calib_dir, f"lidar2img_{frame_idx}.bin")
            lidar2img.astype(np.float32).tofile(calib_path)
            
            print(f"Frame {frame_idx}: Saved lidar2img calibration - shape={lidar2img.shape}")
            
        except StopIteration:
            print(f"Dataset exhausted after {frame_idx} frames")
            break
        except Exception as e:
            print(f"Error processing frame {frame_idx}: {e}")
            continue
    
    print(f"Successfully generated lidar2img calibration files in: {calib_dir}")
    print(f"âœ… Data source consistency with 010 script:")
    print(f"  - Same dataset: test (not val)")
    print(f"  - Same random seed: {seed}")
    print(f"  - Same frame count: {num_frames}")


def generate_sample_data(output_path: str, num_frames: int = 5):
    """
    Generate sample data for testing when nuScenes dataset is not available.
    
    Args:
        output_path: Output directory for sample data
        num_frames: Number of frames to generate
    """
    print(f"Generating sample data for {num_frames} frames...")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    camera_types = ["CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_FRONT_LEFT", 
                   "CAM_BACK", "CAM_BACK_LEFT", "CAM_BACK_RIGHT"]
    
    for cam_type in camera_types:
        cam_dir = os.path.join(output_path, "images", cam_type)
        os.makedirs(cam_dir, exist_ok=True)
    
    calib_dir = os.path.join(output_path, "calib")
    os.makedirs(calib_dir, exist_ok=True)
    
    lidar_dir = os.path.join(output_path, "lidar")
    os.makedirs(lidar_dir, exist_ok=True)
    
    labels_dir = os.path.join(output_path, "labels")
    os.makedirs(labels_dir, exist_ok=True)
    
    temporal_dir = os.path.join(output_path, "temporal")
    os.makedirs(temporal_dir, exist_ok=True)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    for frame_idx in range(num_frames):
        # ä¸ºæ¯ä¸ªç›¸æœºç”Ÿæˆç¤ºä¾‹å›¾åƒ
        for cam_idx, cam_type in enumerate(camera_types):
            img_path = os.path.join(output_path, "images", cam_type, f"{frame_idx}.jpg")
            
            # åˆ›å»ºç¤ºä¾‹å›¾åƒï¼ˆ900x1600ï¼Œä¸åŸå§‹å›¾åƒå°ºå¯¸ä¸€è‡´ï¼‰
            sample_img = np.ones((900, 1600, 3), dtype=np.uint8) * 50
            
            # æ·»åŠ ä¸€äº›æœ‰æ„ä¹‰çš„è§†è§‰å†…å®¹
            # æ·»åŠ èƒŒæ™¯æ¸å˜
            for i in range(900):
                intensity = int(50 + (i / 900) * 100)
                sample_img[i, :, :] = intensity
            
            # æ·»åŠ ä¸€äº›å‡ ä½•å›¾å½¢
            cv2.rectangle(sample_img, (200, 100), (400, 300), (255, 0, 0), 3)
            cv2.circle(sample_img, (800, 450), 100, (0, 255, 0), -1)
            cv2.line(sample_img, (100, 400), (1500, 400), (0, 0, 255), 5)
            
            # æ·»åŠ æ–‡å­—æ ‡è¯†
            cv2.putText(sample_img, f"Frame {frame_idx}", (100, 100), 
                       cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)
            cv2.putText(sample_img, f"Camera {cam_type}", (100, 800), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 2)
            
            cv2.imwrite(img_path, sample_img)
        
        # ç”Ÿæˆç¤ºä¾‹é›·è¾¾ç‚¹äº‘æ•°æ®
        lidar_path = os.path.join(output_path, "lidar", f"lidar_{frame_idx}.bin")
        
        # ç”Ÿæˆéšæœºç‚¹äº‘æ•°æ®ï¼ˆ1000ä¸ªç‚¹ï¼‰
        num_points = 1000
        points = np.random.randn(num_points, 5).astype(np.float32)
        points[:, :3] *= 50.0  # ç¼©æ”¾ä½ç½®åˆ°åˆç†èŒƒå›´
        points[:, 3] = np.random.uniform(0, 255, num_points)  # å¼ºåº¦
        points[:, 4] = np.random.randint(0, 64, num_points)   # ring
        
        points.tofile(lidar_path)
        
        # ç”Ÿæˆç¤ºä¾‹3Dæ ‡ç­¾æ•°æ®
        labels_path = os.path.join(output_path, "labels", f"labels_{frame_idx}.json")
        
        # ç”Ÿæˆä¸€äº›ç¤ºä¾‹3Dç›®æ ‡
        sample_labels = []
        for i in range(np.random.randint(3, 8)):  # 3-7ä¸ªç›®æ ‡
            label_info = {
                'token': f'sample_token_{frame_idx}_{i}',
                'original_category': np.random.choice(list(nuscenes_to_target_mapping.keys())),
                'category_name': np.random.choice(class_names),
                'translation': [np.random.uniform(-20, 20), np.random.uniform(-20, 20), np.random.uniform(-2, 2)],
                'size': [np.random.uniform(3, 6), np.random.uniform(1.5, 2.5), np.random.uniform(1.5, 2.5)],
                'rotation': [1.0, 0.0, 0.0, 0.0],  # å•ä½å››å…ƒæ•°
                'bbox_3d': [0.0] * 7,  # å ä½ç¬¦
                'num_lidar_pts': np.random.randint(10, 100),
                'num_radar_pts': np.random.randint(0, 10),
                'visibility_token': 'visible',
                'instance_token': f'instance_{frame_idx}_{i}',
                'attribute_tokens': []
            }
            sample_labels.append(label_info)
        
        with open(labels_path, 'w') as f:
            json.dump(sample_labels, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç¤ºä¾‹æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯
        temporal_path = os.path.join(output_path, "temporal", f"temporal_{frame_idx}.json")
        
        # æ¨¡æ‹Ÿæ—¶é—´æˆ³ï¼ˆä»0å¼€å§‹ï¼Œæ¯å¸§é—´éš”0.1ç§’ï¼‰
        timestamp = frame_idx * 0.1
        
        # æ¨¡æ‹Ÿego poseï¼ˆè½¦è¾†åœ¨é“è·¯ä¸Šç§»åŠ¨ï¼‰
        ego_translation = [frame_idx * 10.0, 0.0, 0.0]  # æ²¿xè½´å‰è¿›
        ego_rotation = [1.0, 0.0, 0.0, 0.0]  # å•ä½å››å…ƒæ•°
        
        # æ¨¡æ‹Ÿlidar2egoå˜æ¢ï¼ˆæ¿€å…‰é›·è¾¾åœ¨è½¦é¡¶ï¼‰
        lidar2ego_translation = [0.0, 0.0, 1.8]  # è½¦é¡¶é«˜åº¦
        lidar2ego_rotation = [1.0, 0.0, 0.0, 0.0]  # å•ä½å››å…ƒæ•°
        
        # æ„å»ºå˜æ¢çŸ©é˜µ
        lidar2ego_matrix = np.eye(4)
        lidar2ego_matrix[:3, 3] = lidar2ego_translation
        
        ego2global_matrix = np.eye(4)
        ego2global_matrix[:3, 3] = ego_translation
        
        lidar2global_matrix = ego2global_matrix @ lidar2ego_matrix
        global2lidar_matrix = np.linalg.inv(lidar2global_matrix)
        
        temporal_info = {
            'timestamp_info': {
                'timestamp': timestamp,
                'sample_token': f'sample_{frame_idx}',
                'scene_token': f'scene_{frame_idx // 10}',  # æ¯10å¸§ä¸€ä¸ªåœºæ™¯
                'lidar_token': f'lidar_{frame_idx}'
            },
            'ego_pose_info': {
                'ego2global_translation': ego_translation,
                'ego2global_rotation': ego_rotation,
                'ego_pose_token': f'ego_pose_{frame_idx}'
            },
            'lidar2ego_info': {
                'lidar2ego_translation': lidar2ego_translation,
                'lidar2ego_rotation': lidar2ego_rotation,
                'calibrated_sensor_token': f'calib_{frame_idx}'
            },
            'transform_info': {
                'lidar2ego_matrix': lidar2ego_matrix.tolist(),
                'ego2global_matrix': ego2global_matrix.tolist(),
                'lidar2global_matrix': lidar2global_matrix.tolist(),
                'global2lidar_matrix': global2lidar_matrix.tolist()
            }
        }
        
        with open(temporal_path, 'w') as f:
            json.dump(temporal_info, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç¤ºä¾‹lidar2imgæ ‡å®šå‚æ•° - æ¯å¸§å•ç‹¬ä¿å­˜
        calib_path = os.path.join(output_path, "calib", f"lidar2img_{frame_idx}.bin")
        
        # åˆ›å»º6ä¸ªç›¸æœºçš„4x4å˜æ¢çŸ©é˜µ
        lidar2img_params = np.zeros((6, 4, 4), dtype=np.float32)
        
        for cam_idx in range(6):
            # è®¾ç½®å•ä½çŸ©é˜µä½œä¸ºåŸºç¡€
            lidar2img_params[cam_idx] = np.eye(4)
            
            # ä¸ºä¸åŒç›¸æœºæ·»åŠ ä¸åŒçš„åç§»
            lidar2img_params[cam_idx, 0, 3] = cam_idx * 0.1  # xåç§»
            lidar2img_params[cam_idx, 1, 3] = cam_idx * 0.05 # yåç§»
            lidar2img_params[cam_idx, 2, 3] = 1.0 + cam_idx * 0.02 # zåç§»
        
        # ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
        lidar2img_params.tofile(calib_path)
    
    print(f"Sample data generated successfully in: {output_path}")
    print(f"Generated {num_frames} frames with complete data including:")
    print(f"  - Images: {len(camera_types)} cameras per frame")
    print(f"  - Lidar point clouds: {num_points} points per frame")
    print(f"  - 3D labels: 3-7 objects per frame")
    print(f"  - Temporal info: timestamp and transform matrices")
    print(f"  - Calibration: lidar2img matrices")


def generate_nuscenes_offline_data_direct(config_path: str, output_path: str, num_frames: int = 10):
    """
    Generate nuScenes format offline data directly using nuScenes API.
    
    Args:
        config_path: Path to the dataset config file
        output_path: Output directory for offline data
        num_frames: Number of frames to generate
    """
    print(f"Generating nuScenes offline data directly for {num_frames} frames...")
    
    try:
        from nuscenes import NuScenes
        from nuscenes.utils.data_classes import Box
        from nuscenes.utils.geometry_utils import view_points
    except ImportError as e:
        print(f"âŒ nuScenes not available: {e}")
        print("Generating sample data instead...")
        generate_sample_data(output_path, num_frames)
        return
    
    # è¯»å–é…ç½®
    cfg = read_cfg(config_path)
    dataset_cfg = cfg["data"]["val"].copy()
    
    # è·å–nuScenesæ•°æ®é›†è·¯å¾„
    data_root = dataset_cfg.get("data_root", "")
    version = dataset_cfg.get("version", "v1.0-trainval")
    
    print(f"Data root: {data_root}")
    print(f"Version: {version}")
    
    if not data_root or not os.path.exists(data_root):
        print(f"âŒ Data root not found: {data_root}")
        print("Generating sample data instead...")
        generate_sample_data(output_path, num_frames)
        return
    
    try:
        # åˆå§‹åŒ–nuScenes
        nusc = NuScenes(version=version, dataroot=data_root, verbose=True)
        print(f"âœ… NuScenes loaded successfully with {len(nusc.sample)} samples")
    except Exception as e:
        print(f"âŒ Error loading NuScenes: {e}")
        print("Generating sample data instead...")
        generate_sample_data(output_path, num_frames)
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    camera_types = ["CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_FRONT_LEFT", 
                   "CAM_BACK", "CAM_BACK_LEFT", "CAM_BACK_RIGHT"]
    
    for cam_type in camera_types:
        cam_dir = os.path.join(output_path, "images", cam_type)
        os.makedirs(cam_dir, exist_ok=True)
    
    calib_dir = os.path.join(output_path, "calib")
    os.makedirs(calib_dir, exist_ok=True)
    
    # åˆ›å»ºé›·è¾¾æ•°æ®å’Œæ ‡ç­¾æ•°æ®ç›®å½•
    lidar_dir = os.path.join(output_path, "lidar")
    os.makedirs(lidar_dir, exist_ok=True)
    
    labels_dir = os.path.join(output_path, "labels")
    os.makedirs(labels_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰æ ·æœ¬
    samples = list(nusc.sample)
    print(f"Found {len(samples)} samples in dataset")
    
    # å¤„ç†å‰num_framesä¸ªæ ·æœ¬
    for frame_idx in tqdm(range(min(num_frames, len(samples))), desc="Generating frames"):
        sample = samples[frame_idx]
        print(f"\nProcessing frame {frame_idx}: {sample['token']}")
        
        # è·å–è¯¥æ ·æœ¬çš„æ‰€æœ‰ç›¸æœºæ•°æ®
        sample_data = nusc.get('sample_data', sample['data']['CAM_FRONT'])
        
        # è·å–æ‰€æœ‰ç›¸æœºçš„å›¾åƒ
        camera_data = {}
        for cam_type in camera_types:
            if cam_type in sample['data']:
                camera_data[cam_type] = nusc.get('sample_data', sample['data'][cam_type])
        
        print(f"Found {len(camera_data)} cameras for this sample")
        
        # å¤„ç†æ¯ä¸ªç›¸æœº
        for cam_type in camera_types:
            if cam_type in camera_data:
                cam_data = camera_data[cam_type]
                
                # è·å–åŸå§‹å›¾åƒæ–‡ä»¶è·¯å¾„
                original_img_path = os.path.join(data_root, cam_data['filename'])
                target_img_path = os.path.join(output_path, "images", cam_type, f"{frame_idx}.jpg")
                
                print(f"  Camera {cam_type}: {original_img_path}")
                
                # æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(original_img_path):
                    # ç›´æ¥å¤åˆ¶åŸå§‹å›¾åƒæ–‡ä»¶
                    import shutil
                    shutil.copy2(original_img_path, target_img_path)
                    print(f"    âœ… Copied from {original_img_path} to {target_img_path}")
                    
                    # éªŒè¯å¤åˆ¶çš„å›¾åƒ
                    try:
                        img = cv2.imread(target_img_path)
                        print(f"    ğŸ“Š Image stats: shape={img.shape}, mean={img.mean():.1f}, std={img.std():.1f}")
                    except Exception as e:
                        print(f"    âŒ Error reading copied image: {e}")
                else:
                    print(f"    âŒ Original file not found: {original_img_path}")
                    # ç”Ÿæˆç¤ºä¾‹å›¾åƒ
                    sample_img = np.ones((900, 1600, 3), dtype=np.uint8) * 50
                    cv2.putText(sample_img, f"Missing: {os.path.basename(original_img_path)}", 
                               (100, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                    cv2.imwrite(target_img_path, sample_img)
                    print(f"    âš ï¸ Generated sample image instead")
            else:
                print(f"  Camera {cam_type}: No data available")
        
        # è·å–é›·è¾¾ç‚¹äº‘æ•°æ®
        try:
            if 'LIDAR_TOP' in sample['data']:
                lidar_data = nusc.get('sample_data', sample['data']['LIDAR_TOP'])
                lidar_file_path = os.path.join(data_root, lidar_data['filename'])
                target_lidar_path = os.path.join(output_path, "lidar", f"lidar_{frame_idx}.bin")
                
                print(f"  LIDAR_TOP: {lidar_file_path}")
                
                if os.path.exists(lidar_file_path):
                    # å¤åˆ¶é›·è¾¾ç‚¹äº‘æ–‡ä»¶
                    shutil.copy2(lidar_file_path, target_lidar_path)
                    print(f"    âœ… Copied lidar from {lidar_file_path} to {target_lidar_path}")
                    
                    # è¯»å–å¹¶æ˜¾ç¤ºç‚¹äº‘ç»Ÿè®¡ä¿¡æ¯
                    try:
                        points = np.fromfile(lidar_file_path, dtype=np.float32).reshape(-1, 5)
                        print(f"    ğŸ“Š Lidar stats: {points.shape[0]} points, range=[{points[:, :3].min(axis=0)}, {points[:, :3].max(axis=0)}]")
                    except Exception as e:
                        print(f"    âŒ Error reading lidar: {e}")
                else:
                    print(f"    âŒ Lidar file not found: {lidar_file_path}")
            else:
                print(f"  LIDAR_TOP: No data available")
        except Exception as e:
            print(f"  âŒ Error processing lidar: {e}")
        
        # è·å–3Dæ ‡ç­¾æ•°æ®
        try:
            # è·å–è¯¥æ ·æœ¬çš„æ‰€æœ‰3Dæ ‡æ³¨ - ä½¿ç”¨ä¸dump_nusc_ann_json.pyç›¸åŒçš„æ–¹å¼
            sample_annotations = []
            if "anns" in sample and sample["anns"]:
                sample_annotations = [
                    nusc.get("sample_annotation", token) for token in sample["anns"]
                ]
            
            print(f"  Labels: Found {len(sample_annotations)} annotations")
            
            if sample_annotations:
                # ä½¿ç”¨nuScenes APIè·å–æ¿€å…‰é›·è¾¾åæ ‡ç³»ä¸‹çš„boxesï¼ˆä¸dump_nusc_ann_json.pyä¸€è‡´ï¼‰
                lidar_token = sample['data']['LIDAR_TOP']
                _, boxes, _ = nusc.get_sample_data(lidar_token)
                
                # ä¿å­˜æ ‡ç­¾æ•°æ®ä¸ºJSONæ ¼å¼ï¼Œä¾¿äºå¯è§†åŒ–
                labels_data = []
                for i, ann in enumerate(sample_annotations):
                    try:
                        # è·å–åŸå§‹ç±»åˆ«åç§°
                        original_category = ann['category_name']
                        
                        # æ˜ å°„åˆ°ç›®æ ‡ç±»åˆ«
                        if original_category in nuscenes_to_target_mapping:
                            target_category = nuscenes_to_target_mapping[original_category]
                        else:
                            # å¦‚æœä¸åœ¨æ˜ å°„ä¸­ï¼Œè·³è¿‡è¿™ä¸ªæ ‡æ³¨
                            print(f"    âš ï¸ Skipping unknown category: {original_category}")
                            continue
                        
                        # ä½¿ç”¨boxesä¸­çš„æ•°æ®ï¼ˆå·²ç»æ˜¯æ¿€å…‰é›·è¾¾åæ ‡ç³»ï¼‰
                        if i < len(boxes):
                            box = boxes[i]
                            # è·å–ä½ç½®ã€å°ºå¯¸ã€æ—‹è½¬ï¼ˆä¸dump_nusc_ann_json.pyä¸€è‡´ï¼‰
                            loc = box.center      # [x, y, z]
                            dim = box.wlh         # [w, l, h]
                            rot = box.orientation.yaw_pitch_roll[0]  # yawè§’
                            
                            # æ„å»ºbbox_3dæ•°ç»„ [x, y, z, l, w, h, yaw]ï¼ˆä¸dump_nusc_ann_json.pyä¸€è‡´ï¼‰
                            bbox = [
                                float(loc[0]),    # x
                                float(loc[1]),    # y  
                                float(loc[2]),    # z
                                float(dim[1]),    # l (length)
                                float(dim[0]),    # w (width)
                                float(dim[2]),    # h (height)
                                float(rot)        # yaw
                            ]
                            
                            label_info = {
                                'token': ann['token'],
                                'original_category': original_category,  # ä¿ç•™åŸå§‹ç±»åˆ«ç”¨äºè°ƒè¯•
                                'category_name': target_category,        # ä½¿ç”¨æ˜ å°„åçš„ç±»åˆ«
                                'translation': loc.tolist(),             # æ¿€å…‰é›·è¾¾åæ ‡ç³»ä¸‹çš„åæ ‡ [x, y, z]
                                'size': dim.tolist(),                    # æ¿€å…‰é›·è¾¾åæ ‡ç³»ä¸‹çš„å°ºå¯¸ [w, l, h]
                                'rotation': box.orientation.elements.tolist(),  # å››å…ƒæ•° [w, x, y, z]
                                'bbox_3d': bbox,                         # [x, y, z, l, w, h, yaw]
                                'num_lidar_pts': ann.get('num_lidar_pts', 0),
                                'num_radar_pts': ann.get('num_radar_pts', 0),
                                'visibility_token': ann.get('visibility_token', ''),
                                'instance_token': ann.get('instance_token', ''),
                                'attribute_tokens': ann.get('attribute_tokens', [])
                            }
                            labels_data.append(label_info)
                            # print(f"    ğŸ“ Added label: {original_category} -> {target_category}")
                            # print(f"        Lidar coords: {loc}, size: {dim}, yaw: {rot:.3f}")
                        else:
                            print(f"    âš ï¸ Box index {i} out of range for annotation {ann.get('token', 'unknown')}")
                            
                    except Exception as e:
                        print(f"    âš ï¸ Error processing annotation {ann.get('token', 'unknown')}: {e}")
                        continue
                
                if labels_data:
                    # ä¿å­˜æ ‡ç­¾æ•°æ®
                    labels_file_path = os.path.join(output_path, "labels", f"labels_{frame_idx}.json")
                    try:
                        with open(labels_file_path, 'w') as f:
                            json.dump(labels_data, f, indent=2, ensure_ascii=False)
                        
                        print(f"    âœ… Saved {len(labels_data)} 3D labels to {labels_file_path}")
                        
                        # æ˜¾ç¤ºæ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯
                        categories = [label['category_name'] for label in labels_data]  # ä½¿ç”¨æ˜ å°„åçš„ç±»åˆ«
                        from collections import Counter
                        category_counts = Counter(categories)
                        print(f"    ğŸ“Š Label stats: {dict(category_counts)}")
                        
                        # æ˜¾ç¤ºåŸå§‹ç±»åˆ«ç»Ÿè®¡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        original_categories = [label['original_category'] for label in labels_data]
                        original_counts = Counter(original_categories)
                        print(f"   Original categories: {dict(original_counts)}")
                    except Exception as e:
                        print(f"    âŒ Error saving labels to {labels_file_path}: {e}")
                        import traceback
                        traceback.print_exc()
                else:
                    print(f"    âš ï¸ No valid labels to save")
            else:
                print(f"    âš ï¸ No 3D annotations found for this sample")
                
        except Exception as e:
            print(f"  âŒ Error processing labels: {e}")
            import traceback
            traceback.print_exc()
        
        # è·å–æ ‡å®šå‚æ•°
        try:
            # è·å–æ¿€å…‰é›·è¾¾çš„æ ‡å®šä¿¡æ¯
            lidar_data = nusc.get('sample_data', sample['data']['LIDAR_TOP'])
            lidar_sensor = nusc.get('calibrated_sensor', lidar_data['calibrated_sensor_token'])
            lidar_pose = nusc.get('ego_pose', lidar_data['ego_pose_token'])
            
            # ä¿å­˜æ—¶é—´æˆ³å’Œç©ºé—´å¯¹é½ä¿¡æ¯
            timestamp_info = {
                'timestamp': sample['timestamp'] / 1e6,  # è½¬æ¢ä¸ºç§’
                'sample_token': sample['token'],
                'scene_token': sample['scene_token'],
                'lidar_token': lidar_data['token']
            }
            
            # ä¿å­˜ego poseä¿¡æ¯ï¼ˆç”¨äºç©ºé—´å¯¹é½ï¼‰
            ego_pose_info = {
                'ego2global_translation': lidar_pose['translation'],  # [x, y, z]
                'ego2global_rotation': lidar_pose['rotation'],        # [w, x, y, z] quaternion
                'ego_pose_token': lidar_pose['token']
            }
            
            # ä¿å­˜lidar2egoå˜æ¢ä¿¡æ¯
            lidar2ego_info = {
                'lidar2ego_translation': lidar_sensor['translation'],  # [x, y, z]
                'lidar2ego_rotation': lidar_sensor['rotation'],        # [w, x, y, z] quaternion
                'calibrated_sensor_token': lidar_sensor['token']
            }
            
            # è®¡ç®—å¹¶ä¿å­˜å®Œæ•´çš„å˜æ¢çŸ©é˜µ
            from pyquaternion import Quaternion
            
            # æ„å»ºlidar2egoå˜æ¢çŸ©é˜µ
            lidar2ego_R = Quaternion(lidar_sensor['rotation']).rotation_matrix
            lidar2ego_T = np.array(lidar_sensor['translation'])
            lidar2ego_matrix = np.eye(4)
            lidar2ego_matrix[:3, :3] = lidar2ego_R
            lidar2ego_matrix[:3, 3] = lidar2ego_T
            
            # æ„å»ºego2globalå˜æ¢çŸ©é˜µ
            ego2global_R = Quaternion(lidar_pose['rotation']).rotation_matrix
            ego2global_T = np.array(lidar_pose['translation'])
            ego2global_matrix = np.eye(4)
            ego2global_matrix[:3, :3] = ego2global_R
            ego2global_matrix[:3, 3] = ego2global_T
            
            # è®¡ç®—lidar2globalå˜æ¢çŸ©é˜µ
            lidar2global_matrix = ego2global_matrix @ lidar2ego_matrix
            
            # è®¡ç®—global2lidarå˜æ¢çŸ©é˜µï¼ˆç”¨äºç©ºé—´å¯¹é½ï¼‰
            global2lidar_matrix = np.linalg.inv(lidar2global_matrix)
            
            # ä¿å­˜å˜æ¢çŸ©é˜µä¿¡æ¯
            transform_info = {
                'lidar2ego_matrix': lidar2ego_matrix.tolist(),
                'ego2global_matrix': ego2global_matrix.tolist(),
                'lidar2global_matrix': lidar2global_matrix.tolist(),
                'global2lidar_matrix': global2lidar_matrix.tolist()
            }
            
            # ä¿å­˜æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯åˆ°JSONæ–‡ä»¶
            temporal_info = {
                'timestamp_info': timestamp_info,
                'ego_pose_info': ego_pose_info,
                'lidar2ego_info': lidar2ego_info,
                'transform_info': transform_info
            }
            
            temporal_file_path = os.path.join(output_path, "temporal", f"temporal_{frame_idx}.json")
            os.makedirs(os.path.dirname(temporal_file_path), exist_ok=True)
            
            try:
                with open(temporal_file_path, 'w') as f:
                    json.dump(temporal_info, f, indent=2, ensure_ascii=False)
                print(f"  âœ… Saved temporal info to {temporal_file_path}")
                print(f"    ğŸ“Š Timestamp: {timestamp_info['timestamp']:.6f}s")
                print(f"    ğŸ“ Ego position: {ego_pose_info['ego2global_translation']}")
            except Exception as e:
                print(f"    âŒ Error saving temporal info: {e}")
            
            # ä¸ºæ¯ä¸ªç›¸æœºæ„å»ºlidar2imgå˜æ¢çŸ©é˜µ
            all_lidar2img_matrices = []
            
            for cam_type in camera_types:
                if cam_type in camera_data:
                    cam_data = camera_data[cam_type]
                    
                    try:
                        # ä½¿ç”¨nuScenesçš„get_sample_dataæ–¹æ³•è·å–æ­£ç¡®çš„å˜æ¢
                        # è¿™ä¸ªæ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰åæ ‡ç³»å˜æ¢
                        _, _, cam_intrinsic_matrix = nusc.get_sample_data(cam_data['token'])
                        
                        # è·å–lidaråˆ°ç›¸æœºçš„å˜æ¢
                        # ä½¿ç”¨nuScenesçš„view_pointsæ–¹æ³•éœ€è¦çš„å˜æ¢
                        cam_sensor = nusc.get('calibrated_sensor', cam_data['calibrated_sensor_token'])
                        cam_pose = nusc.get('ego_pose', cam_data['ego_pose_token'])
                        
                        # æ„å»ºå®Œæ•´çš„å˜æ¢çŸ©é˜µé“¾
                        from pyquaternion import Quaternion
                        
                        # 1. æ¿€å…‰é›·è¾¾åˆ°egoçš„å˜æ¢
                        lidar2ego_R = Quaternion(lidar_sensor['rotation']).rotation_matrix
                        lidar2ego_T = np.array(lidar_sensor['translation'])
                        lidar2ego = np.eye(4)
                        lidar2ego[:3, :3] = lidar2ego_R
                        lidar2ego[:3, 3] = lidar2ego_T
                        
                        # 2. egoåˆ°globalçš„å˜æ¢ï¼ˆæ¿€å…‰é›·è¾¾å¸§ï¼‰
                        ego2global_R = Quaternion(lidar_pose['rotation']).rotation_matrix
                        ego2global_T = np.array(lidar_pose['translation'])
                        ego2global = np.eye(4)
                        ego2global[:3, :3] = ego2global_R
                        ego2global[:3, 3] = ego2global_T
                        
                        # 3. globalåˆ°egoçš„å˜æ¢ï¼ˆç›¸æœºå¸§ï¼‰
                        cam_ego2global_R = Quaternion(cam_pose['rotation']).rotation_matrix
                        cam_ego2global_T = np.array(cam_pose['translation'])
                        cam_ego2global = np.eye(4)
                        cam_ego2global[:3, :3] = cam_ego2global_R
                        cam_ego2global[:3, 3] = cam_ego2global_T
                        
                        # è®¡ç®—globalåˆ°egoçš„é€†å˜æ¢
                        global2cam_ego = np.linalg.inv(cam_ego2global)
                        
                        # 4. egoåˆ°ç›¸æœºçš„å˜æ¢
                        cam2ego_R = Quaternion(cam_sensor['rotation']).rotation_matrix
                        cam2ego_T = np.array(cam_sensor['translation'])
                        cam2ego = np.eye(4)
                        cam2ego[:3, :3] = cam2ego_R
                        cam2ego[:3, 3] = cam2ego_T
                        
                        # 5. ç›¸æœºå†…å‚çŸ©é˜µ
                        cam_intrinsic = np.array(cam_sensor['camera_intrinsic'])
                        intrinsic_4x4 = np.eye(4)
                        intrinsic_4x4[:3, :3] = cam_intrinsic
                        
                        # ç»„åˆå˜æ¢çŸ©é˜µï¼šlidar -> ego -> global -> ego' -> camera -> image
                        # lidar2img = intrinsic_4x4 @ cam2ego @ global2cam_ego @ ego2global @ lidar2ego
                        lidar2img_4x4 = intrinsic_4x4 @ cam2ego @ global2cam_ego @ ego2global @ lidar2ego
                        
                        print(f"    ğŸ“ {cam_type}: å†…å‚çŸ©é˜µ")
                        print(f"        fx={cam_intrinsic[0,0]:.1f}, fy={cam_intrinsic[1,1]:.1f}")
                        print(f"        cx={cam_intrinsic[0,2]:.1f}, cy={cam_intrinsic[1,2]:.1f}")
                        
                    except Exception as e:
                        print(f"    âš ï¸ Error computing exact transform for {cam_type}: {e}")
                        # ä½¿ç”¨æ›´åˆç†çš„é»˜è®¤å˜æ¢çŸ©é˜µ
                        lidar2img_4x4 = np.eye(4)
                        
                        # è®¾ç½®ç›¸æœºå†…å‚ï¼ˆåŸºäºnuScenesçš„å…¸å‹å€¼ï¼‰
                        lidar2img_4x4[0, 0] = 1200.0  # fx - ç„¦è·x
                        lidar2img_4x4[1, 1] = 1200.0  # fy - ç„¦è·y
                        lidar2img_4x4[0, 2] = 800.0   # cx - ä¸»ç‚¹x (1600/2)
                        lidar2img_4x4[1, 2] = 450.0   # cy - ä¸»ç‚¹y (900/2)
                        
                        # è®¾ç½®å¤–å‚ï¼ˆæ¿€å…‰é›·è¾¾åˆ°ç›¸æœºçš„å˜æ¢ï¼‰
                        # å‡è®¾æ¿€å…‰é›·è¾¾åœ¨è½¦é¡¶ï¼Œç›¸æœºåœ¨è½¦è¾†å‰æ–¹
                        lidar2img_4x4[0, 3] = 0.0     # xåç§»
                        lidar2img_4x4[1, 3] = 0.0     # yåç§»
                        lidar2img_4x4[2, 3] = -1.5    # zåç§»ï¼ˆæ¿€å…‰é›·è¾¾åˆ°ç›¸æœºçš„é«˜åº¦å·®ï¼‰
                        
                        print(f"    âš ï¸ {cam_type}: ä½¿ç”¨é»˜è®¤å˜æ¢çŸ©é˜µ")
                    
                    all_lidar2img_matrices.append(lidar2img_4x4)
                    print(f"    ğŸ“ {cam_type}: lidar2img matrix computed")
                else:
                    # å¦‚æœç›¸æœºæ•°æ®ä¸å­˜åœ¨ï¼Œä½¿ç”¨å•ä½çŸ©é˜µ
                    all_lidar2img_matrices.append(np.eye(4))
                    print(f"    âš ï¸ {cam_type}: using identity matrix")
            
            # å°†æ‰€æœ‰ç›¸æœºçš„å˜æ¢çŸ©é˜µä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ - æ¯å¸§å•ç‹¬ä¿å­˜
            calib_path = os.path.join(output_path, "calib", f"lidar2img_{frame_idx}.bin")
            all_matrices = np.array(all_lidar2img_matrices, dtype=np.float32)
            all_matrices.tofile(calib_path)
            print(f"  âœ… Saved calibration data for frame {frame_idx}: shape={all_matrices.shape}")
            
        except Exception as e:
            print(f"  âŒ Error getting calibration: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"Successfully generated offline data in: {output_path}")
    print(f"Generated {min(num_frames, len(samples))} frames with {len(camera_types)} cameras each")
    print(f"Data includes: images, lidar point clouds, 3D labels, and calibration data")


def verify_generated_data(output_path: str):
    """
    Verify that the generated data is valid.
    
    Args:
        output_path: Path to the generated data directory
    """
    print(f"Verifying generated data in: {output_path}")
    
    camera_types = ["CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_FRONT_LEFT", 
                   "CAM_BACK", "CAM_BACK_LEFT", "CAM_BACK_RIGHT"]
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    images_dir = os.path.join(output_path, "images")
    if not os.path.exists(images_dir):
        print("âŒ Images directory not found!")
        return False
    
    valid_images = 0
    total_images = 0
    
    for cam_type in camera_types:
        cam_dir = os.path.join(images_dir, cam_type)
        if not os.path.exists(cam_dir):
            print(f"âŒ Camera directory not found: {cam_dir}")
            continue
        
        image_files = [f for f in os.listdir(cam_dir) if f.endswith('.jpg')]
        total_images += len(image_files)
        
        for img_file in image_files:
            img_path = os.path.join(cam_dir, img_file)
            try:
                # å°è¯•è¯»å–å›¾åƒ
                img = cv2.imread(img_path)
                if img is not None and img.size > 0:
                    valid_images += 1
                    print(f"âœ… {img_path}: shape={img.shape}, size={img.size}")
                else:
                    print(f"âŒ {img_path}: Invalid image")
            except Exception as e:
                print(f"âŒ {img_path}: Error reading image - {e}")
    
    # æ£€æŸ¥æ ‡å®šæ–‡ä»¶
    calib_dir = os.path.join(output_path, "calib")
    valid_calibs = 0
    total_calibs = 0
    
    if os.path.exists(calib_dir):
        calib_files = [f for f in os.listdir(calib_dir) if f.endswith('.bin')]
        total_calibs = len(calib_files)
        
        for calib_file in calib_files:
            calib_path = os.path.join(calib_dir, calib_file)
            try:
                # è¯»å–æ ‡å®šæ•°æ®
                calib_data = np.fromfile(calib_path, dtype=np.float32)
                expected_size = 6 * 4 * 4  # 6ä¸ªç›¸æœºï¼Œæ¯ä¸ª4x4çŸ©é˜µ
                if calib_data.size == expected_size:
                    valid_calibs += 1
                    print(f"âœ… {calib_path}: shape={calib_data.shape}")
                else:
                    print(f"âŒ {calib_path}: Wrong size {calib_data.size}, expected {expected_size}")
            except Exception as e:
                print(f"âŒ {calib_path}: Error reading calib - {e}")
    
    # æ£€æŸ¥é›·è¾¾ç‚¹äº‘æ–‡ä»¶
    lidar_dir = os.path.join(output_path, "lidar")
    valid_lidars = 0
    total_lidars = 0
    
    if os.path.exists(lidar_dir):
        lidar_files = [f for f in os.listdir(lidar_dir) if f.endswith('.bin')]
        total_lidars = len(lidar_files)
        
        for lidar_file in lidar_files:
            lidar_path = os.path.join(lidar_dir, lidar_file)
            try:
                # è¯»å–é›·è¾¾ç‚¹äº‘æ•°æ®
                lidar_data = np.fromfile(lidar_path, dtype=np.float32)
                if lidar_data.size > 0 and lidar_data.size % 5 == 0:  # æ¯ä¸ªç‚¹5ä¸ªå€¼ (x, y, z, intensity, ring)
                    valid_lidars += 1
                    num_points = lidar_data.size // 5
                    print(f"âœ… {lidar_path}: {num_points} points, shape={lidar_data.shape}")
                else:
                    print(f"âŒ {lidar_path}: Invalid lidar data size {lidar_data.size}")
            except Exception as e:
                print(f"âŒ {lidar_path}: Error reading lidar - {e}")
    
    # æ£€æŸ¥3Dæ ‡ç­¾æ–‡ä»¶
    labels_dir = os.path.join(output_path, "labels")
    valid_labels = 0
    total_labels = 0
    
    if os.path.exists(labels_dir):
        label_files = [f for f in os.listdir(labels_dir) if f.endswith('.json')]
        total_labels = len(label_files)
        
        for label_file in label_files:
            label_path = os.path.join(labels_dir, label_file)
            try:
                # è¯»å–æ ‡ç­¾æ•°æ®
                with open(label_path, 'r') as f:
                    labels_data = json.load(f)
                
                if isinstance(labels_data, list):
                    valid_labels += 1
                    num_objects = len(labels_data)
                    categories = [obj.get('category_name', 'unknown') for obj in labels_data]  # ä½¿ç”¨æ˜ å°„åçš„ç±»åˆ«
                    from collections import Counter
                    category_counts = Counter(categories)
                    print(f"âœ… {label_path}: {num_objects} objects, categories={dict(category_counts)}")
                    
                    # æ˜¾ç¤ºåŸå§‹ç±»åˆ«ç»Ÿè®¡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    original_categories = [obj.get('original_category', 'unknown') for obj in labels_data]
                    original_counts = Counter(original_categories)
                    print(f"   Original categories: {dict(original_counts)}")
                else:
                    print(f"âŒ {label_path}: Invalid label format")
            except Exception as e:
                print(f"âŒ {label_path}: Error reading labels - {e}")
    
    # æ£€æŸ¥æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯æ–‡ä»¶
    temporal_dir = os.path.join(output_path, "temporal")
    valid_temporal = 0
    total_temporal = 0
    
    if os.path.exists(temporal_dir):
        temporal_files = [f for f in os.listdir(temporal_dir) if f.endswith('.json')]
        total_temporal = len(temporal_files)
        
        for temporal_file in temporal_files:
            temporal_path = os.path.join(temporal_dir, temporal_file)
            try:
                # è¯»å–æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯
                with open(temporal_path, 'r') as f:
                    temporal_data = json.load(f)
                
                if isinstance(temporal_data, dict) and 'timestamp_info' in temporal_data:
                    valid_temporal += 1
                    timestamp = temporal_data['timestamp_info']['timestamp']
                    ego_translation = temporal_data['ego_pose_info']['ego2global_translation']
                    
                    print(f"âœ… {temporal_path}: timestamp={timestamp:.6f}s, ego_pos={ego_translation}")
                    
                    # éªŒè¯å˜æ¢çŸ©é˜µ
                    if 'transform_info' in temporal_data:
                        transform_info = temporal_data['transform_info']
                        if all(key in transform_info for key in ['lidar2ego_matrix', 'ego2global_matrix', 'global2lidar_matrix']):
                            print(f"   ğŸ“ Transform matrices: âœ… All present")
                        else:
                            print(f"   ğŸ“ Transform matrices: âŒ Missing some matrices")
                else:
                    print(f"âŒ {temporal_path}: Invalid temporal data format")
            except Exception as e:
                print(f"âŒ {temporal_path}: Error reading temporal data - {e}")
    
    print(f"\nğŸ“Š Verification Summary:")
    print(f"  Images: {valid_images}/{total_images} valid")
    print(f"  Calibs: {valid_calibs}/{total_calibs} valid")
    print(f"  Lidars: {valid_lidars}/{total_lidars} valid")
    print(f"  Labels: {valid_labels}/{total_labels} valid")
    print(f"  Temporal: {valid_temporal}/{total_temporal} valid")
    
    return valid_images > 0 and valid_calibs > 0 and valid_lidars > 0 and valid_labels > 0 and valid_temporal > 0


if __name__ == "__main__":
    # é…ç½®å‚æ•° - ä¸010è„šæœ¬ä¿æŒä¸€è‡´
    config_path = "dataset/config/sparse4d_temporal_r50_1x1_bs1_256x704_mini.py"
    output_path = "/share/Code/SparseEnd2End/C++/Data/sparse/"
    num_frames = 3  # ä¸010è„šæœ¬ä¸€è‡´ï¼Œå¤„ç†å‰3å¸§
    
    try:
        # å°è¯•ç”ŸæˆçœŸå®çš„nuScenesæ•°æ®
        if os.path.exists(config_path):
            # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„éšæœºç§å­
            set_random_seed(seed=1, deterministic=True)  # ä¸010è„šæœ¬ä¸€è‡´
            
            # ä½¿ç”¨ä¸010è„šæœ¬å®Œå…¨ç›¸åŒçš„æ•°æ®åŠ è½½å™¨å’Œé…ç½®
            print("ğŸ”§ ä½¿ç”¨ä¸010è„šæœ¬å®Œå…¨ç›¸åŒçš„æ•°æ®åŠ è½½å™¨å’Œé…ç½®...")
            
            # è¯»å–é…ç½®
            cfg = read_cfg(config_path)
            
            # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„testé…ç½®
            dataset_cfg = cfg["data"]["test"].copy()  # ä½¿ç”¨testé…ç½®ï¼Œä¸010è„šæœ¬ä¸€è‡´
            
            # ç§»é™¤ä¸åº”è¯¥ä¼ é€’ç»™æ•°æ®é›†çš„å‚æ•°
            samples_per_gpu = dataset_cfg.pop("samples_per_gpu", 1)
            workers_per_gpu = dataset_cfg.pop("workers_per_gpu", 0)
            
            # ä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„test_pipeline
            dataset_type = dataset_cfg.pop("type")
            dataset = eval(dataset_type)(**dataset_cfg)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä½¿ç”¨ä¸010è„šæœ¬ç›¸åŒçš„é…ç½®
            dataloader = dataloader_wrapper(
                dataset,
                samples_per_gpu=samples_per_gpu,  # ä¸010è„šæœ¬ä¸€è‡´
                workers_per_gpu=workers_per_gpu,
                dist=False,
                shuffle=False,  # ä¸010è„šæœ¬ä¸€è‡´
            )
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            camera_types = ["CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_FRONT_LEFT", 
                           "CAM_BACK", "CAM_BACK_LEFT", "CAM_BACK_RIGHT"]
            
            for cam_type in camera_types:
                cam_dir = os.path.join(output_path, "images", cam_type)
                os.makedirs(cam_dir, exist_ok=True)
            
            calib_dir = os.path.join(output_path, "calib")
            os.makedirs(calib_dir, exist_ok=True)
            
            # å¤„ç†æ•°æ®
            data_iter = dataloader.__iter__()
            
            for frame_idx in tqdm(range(num_frames), desc="Generating frames"):
                try:
                    # è·å–æ•°æ® - ä¸010è„šæœ¬å®Œå…¨ç›¸åŒçš„æ–¹å¼
                    data = next(data_iter)
                    data = scatter(data, [0])[0]
                    
                    # è·å–æ•°æ®
                    img_metas = data["img_metas"][0]
                    lidar2img = data["lidar2img"].data[0].cpu().numpy()  # (1, 6, 4, 4) - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
                    img = data["img"].data[0].cpu().numpy()  # (1, 6, 3, 256, 704) - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
                    ori_img = data["ori_img"].data[0].cpu().numpy()  # (1, 6, 3, 900, 1600) - åŸå§‹å›¾åƒ
                    
                    print(f"\nFrame {frame_idx}: Processing {len(camera_types)} cameras")
                    print(f"  ğŸ“Š ori_img shape: {ori_img.shape}")
                    print(f"  ğŸ“Š img shape: {img.shape}")
                    print(f"  ğŸ“Š lidar2img shape: {lidar2img.shape}")
                    
                    # ä¿å­˜åŸå§‹å›¾åƒæ•°æ®ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ - ä¸010è„šæœ¬æ ¼å¼å®Œå…¨ä¸€è‡´
                    # 010è„šæœ¬ä¿å­˜æ ¼å¼: sample_{frame_idx}_ori_imgs_1*6*3*900*1600_uint8.bin
                    ori_imgs_path = os.path.join(output_path, "calib", f"sample_{frame_idx}_ori_imgs_1*6*3*900*1600_uint8.bin")
                    ori_img.astype(np.uint8).tofile(ori_imgs_path)
                    print(f"  âœ… Saved ori_imgs binary: {ori_imgs_path}")
                    
                    # ä¿å­˜é¢„å¤„ç†å›¾åƒæ•°æ®ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ - ä¸010è„šæœ¬æ ¼å¼å®Œå…¨ä¸€è‡´
                    # 010è„šæœ¬ä¿å­˜æ ¼å¼: sample_{frame_idx}_imgs_1*6*3*256*704_float32.bin
                    imgs_path = os.path.join(output_path, "calib", f"sample_{frame_idx}_imgs_1*6*3*256*704_float32.bin")
                    img.astype(np.float32).tofile(imgs_path)
                    print(f"  âœ… Saved imgs binary: {imgs_path}")
                    
                    # åŒæ—¶ä¿å­˜JPGå›¾åƒæ–‡ä»¶ç”¨äºå¯è§†åŒ–
                    for cam_idx, cam_type in enumerate(camera_types):
                        cam_img = ori_img[0, cam_idx]
                        print(f"  Camera {cam_type}: cam_img shape = {cam_img.shape}")
                        
                        # å¤„ç†ä¸åŒçš„å›¾åƒæ ¼å¼
                        if cam_img.shape == (3, 900, 1600):
                            # ä¸‰é€šé“å½©è‰²å›¾åƒ
                            cam_img_cv = np.transpose(cam_img, (1, 2, 0))
                            cam_img_bgr = cv2.cvtColor(cam_img_cv, cv2.COLOR_RGB2BGR)
                        elif cam_img.shape == (900, 1600):
                            # ç°åº¦å›¾åƒï¼Œè½¬æ¢ä¸ºä¸‰é€šé“
                            print(f"    âš ï¸ æ£€æµ‹åˆ°ç°åº¦å›¾åƒï¼Œè½¬æ¢ä¸ºä¸‰é€šé“")
                            cam_img_bgr = cv2.cvtColor(cam_img, cv2.COLOR_GRAY2BGR)
                        elif cam_img.shape == (900, 1600, 3):
                            # å·²ç»æ˜¯BGRæ ¼å¼
                            cam_img_bgr = cam_img
                        else:
                            print(f"    âš ï¸ æœªçŸ¥å›¾åƒæ ¼å¼: {cam_img.shape}ï¼Œå°è¯•è½¬æ¢ä¸ºä¸‰é€šé“")
                            # å°è¯•è½¬æ¢ä¸ºä¸‰é€šé“
                            if cam_img.ndim == 2:
                                cam_img_bgr = cv2.cvtColor(cam_img, cv2.COLOR_GRAY2BGR)
                            elif cam_img.ndim == 3 and cam_img.shape[2] == 3:
                                cam_img_bgr = cam_img
                            else:
                                raise ValueError(f"æ— æ³•å¤„ç†çš„å›¾åƒæ ¼å¼: {cam_img.shape}")
                        
                        target_img_path = os.path.join(output_path, "images", cam_type, f"{frame_idx}.jpg")
                        cv2.imwrite(target_img_path, cam_img_bgr)
                        print(f"  Camera {cam_type}: Saved to {target_img_path}")
                        print(f"    ğŸ“Š Image stats: shape={cam_img_bgr.shape}, mean={cam_img_bgr.mean():.1f}, std={cam_img_bgr.std():.1f}")
                    
                    # ä¿å­˜lidar2imgæ ‡å®šå‚æ•° - ä¸010è„šæœ¬æ ¼å¼ä¸€è‡´
                    calib_path = os.path.join(output_path, "calib", f"lidar2img_{frame_idx}.bin")
                    lidar2img.astype(np.float32).tofile(calib_path)
                    print(f"  âœ… Saved lidar2img calibration for frame {frame_idx}: shape={lidar2img.shape}")
                    
                    # ä¿å­˜æ—¶é—´æˆ³å’Œå˜æ¢ä¿¡æ¯
                    if "timestamp" in data:
                        timestamp = data["timestamp"].data[0].cpu().numpy()  # (1,)
                        timestamp_path = os.path.join(output_path, "calib", f"timestamp_{frame_idx}.bin")
                        timestamp.astype(np.float32).tofile(timestamp_path)
                        print(f"  âœ… Saved timestamp for frame {frame_idx}: shape={timestamp.shape}")
                    
                    # ä¿å­˜global2lidarå˜æ¢çŸ©é˜µ
                    if "global2lidar" in img_metas:
                        global2lidar = img_metas["global2lidar"]
                        global2lidar_path = os.path.join(output_path, "calib", f"global2lidar_{frame_idx}.bin")
                        global2lidar.astype(np.float32).tofile(global2lidar_path)
                        print(f"  âœ… Saved global2lidar for frame {frame_idx}: shape={global2lidar.shape}")
                    
                except StopIteration:
                    print(f"Dataset exhausted after {frame_idx} frames")
                    break
                except Exception as e:
                    print(f"Error processing frame {frame_idx}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            
            print(f"âœ… æˆåŠŸç”Ÿæˆä¸010è„šæœ¬ä¸€è‡´çš„æ•°æ®")
            print(f"âœ… æ•°æ®æºä¸€è‡´æ€§:")
            print(f"  - ç›¸åŒé…ç½®æ–‡ä»¶: {config_path}")
            print(f"  - ç›¸åŒæ•°æ®é›†: test")
            print(f"  - ç›¸åŒéšæœºç§å­: 1")
            print(f"  - ç›¸åŒæ•°æ®åŠ è½½å™¨: dataloader_wrapper")
            print(f"  - ç›¸åŒå¸§æ•°: {num_frames}")
            
        else:
            print(f"Config file not found: {config_path}")
            print("Generating sample data instead...")
            generate_sample_data(output_path, num_frames)
    except Exception as e:
        print(f"Error generating nuScenes data: {e}")
        print("Generating sample data instead...")
        generate_sample_data(output_path, num_frames) 